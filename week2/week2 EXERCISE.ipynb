{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7640f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8289c1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()\n",
    "\n",
    "# As an alternative, if you'd like to use Ollama instead of OpenAI\n",
    "# Check that Ollama is running for you locally (see week1/day2 exercise) then uncomment these next 2 lines\n",
    "# MODEL = \"llama3.2\"\n",
    "# openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a technical expert of all coding languages as well as maths and statistics.\\\n",
    "      Using your expertise you must answer a questions the user has\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba29aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def answer_gpt(question, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response+= chunk.choices[0].delta.content or ''\n",
    "        yield response\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c71738",
   "metadata": {},
   "source": [
    "# Only chat-gpt. adding \n",
    "* groq, \n",
    "* gemini, and \n",
    "* ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb17d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=answer_gpt, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7fb38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "import ollama\n",
    "\n",
    "MODEL = \"llama3.2\"\n",
    "def answer_alpaca(question, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]\n",
    "    stream = ollama.chat(model=MODEL, messages=messages, stream=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response+= chunk['message']['content'] or ''\n",
    "        yield response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49e39ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=answer_alpaca, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12755103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27baa451",
   "metadata": {},
   "outputs": [],
   "source": [
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af1dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caf72bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6058cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_google(question, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]\n",
    "    stream = gemini_via_openai_client.chat.completions.create(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response+= chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49cecf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tejas\\Anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\components\\chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tejas\\Anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py:229: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_choice = gr.Radio(\n",
    "    choices=[\"Llama\", \"Google\", \"GPT\"],\n",
    "    label=\"Choose AI Model\",\n",
    "    value=\"Llama\"\n",
    ")\n",
    "\n",
    "def answer_with_model(question, history, model):\n",
    "    # Convert the generator to a string before returning\n",
    "    response = \"\"\n",
    "    generator = None\n",
    "    \n",
    "    if model == \"Llama\":\n",
    "        generator = answer_alpaca(question, history)\n",
    "    elif model == \"Google\":\n",
    "        generator = answer_google(question, history)\n",
    "    else:\n",
    "        generator = answer_gpt(question, history)\n",
    "    \n",
    "    # Consume the generator to get the final response\n",
    "    for chunk in generator:\n",
    "        response = chunk  # Each chunk is the cumulative response\n",
    "    \n",
    "    return response\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=answer_with_model,\n",
    "    chatbot=gr.Chatbot(),\n",
    "    additional_inputs=[model_choice]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701631c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
